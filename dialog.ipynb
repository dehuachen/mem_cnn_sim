{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "from data_utils import load_dialog_task, vectorize_data, load_candidates, vectorize_candidates, tokenize\n",
    "from six.moves import range, reduce\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn import metrics\n",
    "from torch.autograd import Variable as V\n",
    "\n",
    "from model.mem_cnn_sim import MemCnnSim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare the data \n",
    "\n",
    "def init(data_dir, task_id, OOV=False):\n",
    "    # load candidates\n",
    "    candidates, candid2indx = load_candidates(\n",
    "        data_dir, task_id)\n",
    "    n_cand = len(candidates)\n",
    "    print(\"Candidate Size\", n_cand)\n",
    "    indx2candid = dict(\n",
    "        (candid2indx[key], key) for key in candid2indx)\n",
    "\n",
    "    # load task data\n",
    "    train_data, test_data, val_data = load_dialog_task(\n",
    "        data_dir, task_id, candid2indx, OOV)\n",
    "    data = train_data + test_data + val_data\n",
    "\n",
    "    # build parameters\n",
    "    word_idx, sentence_size, \\\n",
    "    candidate_sentence_size, memory_size, \\\n",
    "    vocab_size = build_vocab(data, candidates)\n",
    "\n",
    "    # Variable(torch.from_numpy(candidates_vec)).view(len(candidates), sentence_size)\n",
    "    candidates_vec = vectorize_candidates(\n",
    "        candidates, word_idx, candidate_sentence_size)\n",
    "\n",
    "    return candid2indx, \\\n",
    "           indx2candid, \\\n",
    "           candidates_vec, \\\n",
    "           word_idx, \\\n",
    "           sentence_size, \\\n",
    "           candidate_sentence_size, \\\n",
    "           memory_size, \\\n",
    "           vocab_size, \\\n",
    "           train_data, test_data, val_data\n",
    "\n",
    "\n",
    "def build_vocab(data, candidates, memory_size=50):\n",
    "    vocab = reduce(lambda x, y: x | y, (set(\n",
    "        list(chain.from_iterable(s)) + q) for s, q, a in data))\n",
    "    vocab |= reduce(lambda x, y: x | y, (set(candidate)\n",
    "                                         for candidate in candidates))\n",
    "    vocab = sorted(vocab)\n",
    "    word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "\n",
    "    max_story_size = max(map(len, (s for s, _, _ in data)))\n",
    "    mean_story_size = int(np.mean([len(s) for s, _, _ in data]))\n",
    "    sentence_size = max(map(len, chain.from_iterable(s for s, _, _ in data)))\n",
    "    candidate_sentence_size = max(map(len, candidates))\n",
    "    query_size = max(map(len, (q for _, q, _ in data)))\n",
    "    memory_size = min(memory_size, max_story_size)\n",
    "    vocab_size = len(word_idx) + 1  # +1 for nil word\n",
    "    sentence_size = max(query_size, sentence_size)  # for the position\n",
    "    # params\n",
    "    print(\"vocab size:\", vocab_size)\n",
    "    print(\"Longest sentence length\", sentence_size)\n",
    "    print(\"Longest candidate sentence length\", candidate_sentence_size)\n",
    "    print(\"Longest story length\", max_story_size)\n",
    "    print(\"Average story length\", mean_story_size)\n",
    "\n",
    "    return word_idx, \\\n",
    "           sentence_size, \\\n",
    "           candidate_sentence_size, \\\n",
    "           memory_size, \\\n",
    "           vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval(utter_batch, memory_batch, answer__batch, dialog_idx, mem_cnn_sim, cuda=False):\n",
    "    mem_cnn_sim.eval()\n",
    "\n",
    "    total_loss = []\n",
    "    preds = []\n",
    "    for start, end in dialog_idx:\n",
    "\n",
    "        loss_per_diaglo = []\n",
    "\n",
    "        for j in range(start, end + 1):\n",
    "\n",
    "            memory = V(torch.from_numpy(memory_batch[j])).unsqueeze(0)\n",
    "            utter = V(torch.from_numpy(utter_batch[j])).unsqueeze(0)\n",
    "\n",
    "            if cuda:\n",
    "                memory = transfer_to_gpu(memory)\n",
    "                utter = transfer_to_gpu(utter)\n",
    "\n",
    "            context, cand_ = mem_cnn_sim(utter, memory, cands_tensor)\n",
    "            pred = mem_cnn_sim.predict(context, cand_)\n",
    "            preds.append(pred.data[0])\n",
    "\n",
    "            print('pred: {}, loss: {}'.format(indx2candid[pred.data[0]], loss.data[0]))\n",
    "\n",
    "            loss_per_diaglo.append(loss.data[0])\n",
    "\n",
    "        total_loss += loss_per_diaglo\n",
    "\n",
    "    accuracy = metrics.accuracy_score(answer__batch[:len(preds)], preds)\n",
    "    print()\n",
    "    print('Validation accuracy: {}'.format(accuracy))\n",
    "    print('Validation loss: {}'.format(sum(total_loss)))\n",
    "    input()\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def transfer_to_gpu(tensor, dtype=torch.LongTensor):\n",
    "    tensor_cuda = dtype(tensor.size()).cuda()\n",
    "    tensor_cuda = V(tensor_cuda)\n",
    "    tensor_cuda.data.copy_(tensor.data)\n",
    "    return tensor_cuda\n",
    "\n",
    "\n",
    "def save_checkpoint(state, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "\n",
    "\n",
    "def load_checkpoit(model, optimizer, path_to_model):\n",
    "    if os.path.isfile(path_to_model):\n",
    "        print(\"=> loading checkpoint '{}'\".format(path_to_model))\n",
    "        checkpoint = torch.load(path_to_model)\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "              .format(path_to_model, checkpoint['epoch']))\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(path_to_model))\n",
    "\n",
    "\n",
    "def load_model(model, model_dir):\n",
    "    load_checkpoit(model, model.optimizer, model_dir+'best_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model(mem_cnn_sim):\n",
    "\n",
    "    for i in range(20):\n",
    "        utter = V(torch.LongTensor([1,1,1])).unsqueeze(0)\n",
    "        memory = V(torch.LongTensor([[1,2,3], [4,5,6]])).unsqueeze(0)\n",
    "        cand = V(torch.LongTensor([[7,8,9], [10,11,12], [13,14,15], [16,17,18]]))\n",
    "        flag = V(torch.FloatTensor([0,0,0,1]))\n",
    "\n",
    "        context, cand_ = mem_cnn_sim(utter, memory, cand)\n",
    "        loss = mem_cnn_sim.loss_op(context, cand_, flag)\n",
    "        pred = mem_cnn_sim.predict(context, cand_)\n",
    "        mem_cnn_sim.optimize(loss)\n",
    "\n",
    "        print('loss: {}, pred: {}'.format(loss.data[0], pred.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chen/anaconda3/envs/pytorch0.2/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate Size 2407\n",
      "vocab size: 1993\n",
      "Longest sentence length 29\n",
      "Longest candidate sentence length 27\n",
      "Longest story length 812\n",
      "Average story length 32\n",
      "Training Size 14404\n",
      "Validation Size 4159\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"data/dialog-bAbI-tasks/\"\n",
    "task_id = 6\n",
    "epochs = 10\n",
    "model_dir = \"task\" + str(task_id) + \"_model/\"\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "test_ = False\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "if cuda: print('Cuda is available.')\n",
    "\n",
    "candid2indx, \\\n",
    "indx2candid, \\\n",
    "candidates_vec, \\\n",
    "word_idx, \\\n",
    "sentence_size, \\\n",
    "candidate_sentence_size, \\\n",
    "memory_size, \\\n",
    "vocab_size, \\\n",
    "train_data, test_data, val_data = init(data_dir, task_id)\n",
    "\n",
    "trainS, trainQ, trainA, dialog_idx = vectorize_data(\n",
    "    train_data, word_idx, sentence_size, memory_size)\n",
    "valS, valQ, valA, dialog_idx_val = vectorize_data(\n",
    "    val_data, word_idx, sentence_size, memory_size)\n",
    "n_train = len(trainS)\n",
    "n_val = len(valS)\n",
    "\n",
    "print(\"Training Size\", n_train)\n",
    "print(\"Validation Size\", n_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param = {\n",
    "        'hops': 3,\n",
    "        \"vocab_size\": vocab_size,\n",
    "        \"embedding_size\": 80,\n",
    "        'num_filters': 20,\n",
    "        \"cand_vocab_size\": vocab_size,\n",
    "        'max_grad_norm': 40.0\n",
    "         }\n",
    "\n",
    "mem_cnn_sim = MemCnnSim(param)\n",
    "\n",
    "if test_:\n",
    "    test_model(mem_cnn_sim)\n",
    "    input()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_validation_accuracy = 0\n",
    "time = []\n",
    "\n",
    "cands_tensor = V(torch.from_numpy(candidates_vec))\n",
    "num_cand = cands_tensor.size(0)\n",
    "num_dialog = len(dialog_idx)\n",
    "\n",
    "if cuda:\n",
    "    cands_tensor = transfer_to_gpu(cands_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MemCnnSim (\n",
       "  (memn2n): CNN (\n",
       "    (embedding): Embedding(1993, 80)\n",
       "    (cnn): Conv2d(1, 20, kernel_size=(2, 80), stride=(1, 1))\n",
       "    (l1): Linear (20 -> 80)\n",
       "    (l2): Linear (80 -> 80)\n",
       "    (l3): Linear (80 -> 80)\n",
       "  )\n",
       "  (cnn): CNN (\n",
       "    (embedding): Embedding(1993, 80)\n",
       "    (cnn): Conv2d(1, 20, kernel_size=(2, 80), stride=(1, 1))\n",
       "    (l1): Linear (20 -> 80)\n",
       "    (l2): Linear (80 -> 80)\n",
       "    (l3): Linear (80 -> 80)\n",
       "  )\n",
       "  (criterion): CosineEmbeddingLoss (\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem_cnn_sim.optimizer = optim.Adam(mem_cnn_sim.parameters(), lr=0.01)\n",
    "mem_cnn_sim.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0  # dialog utter\n",
    "for k in range(20):\n",
    "    ans = trainA[k]\n",
    "\n",
    "    memory = V(torch.from_numpy(trainS[k])).unsqueeze(0)\n",
    "    utter = V(torch.from_numpy(trainQ[k])).unsqueeze(0)\n",
    "\n",
    "    flag = -1 * torch.ones(num_cand)\n",
    "    flag[ans] = 1\n",
    "\n",
    "    flag = V(flag)\n",
    "\n",
    "    if cuda:\n",
    "        mem_cnn_sim.cuda()\n",
    "\n",
    "        memory = transfer_to_gpu(memory)\n",
    "        utter = transfer_to_gpu(utter)\n",
    "        flag = transfer_to_gpu(flag, dtype=torch.FloatTensor)\n",
    "\n",
    "    context, cand_ = mem_cnn_sim(utter, memory, cands_tensor)\n",
    "    loss = mem_cnn_sim.loss_op(context, cand_, flag)\n",
    "    mem_cnn_sim.optimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The price range at curry_garden is expensive .\n",
      "1306\n",
      "0.00037769321352243423\n"
     ]
    }
   ],
   "source": [
    "context, cand_ = mem_cnn_sim(utter, memory, cands_tensor)\n",
    "pred = mem_cnn_sim.predict(cand_=cand_, context=context)\n",
    "print(indx2candid[pred.data[0]])\n",
    "print(pred.data[0])\n",
    "print(loss.data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([], ['<silence>'], 1603)\n",
      "1603\n",
      "[812   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0]\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[k])\n",
    "print(trainA[k])\n",
    "print(trainQ[k])\n",
    "print(trainS[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch0.2",
   "language": "python",
   "name": "pytorch0.2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
